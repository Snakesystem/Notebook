{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series - append:\n",
      " 0    a\n",
      "1    b\n",
      "2    c\n",
      "3    d\n",
      "4    e\n",
      "5    f\n",
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "4    5\n",
      "5    6\n",
      "dtype: object\n",
      "Dataframe - append:\n",
      "    b  a\n",
      "0  1  3\n",
      "1  2  4\n",
      "0  3  1\n",
      "1  4  2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Buat series of int (s1) dan series of string (s2)\n",
    "s1 = pd.Series([1,2,3,4,5,6])\n",
    "s2 = pd.Series([\"a\",\"b\",\"c\",\"d\",\"e\",\"f\"])\n",
    "# Terapkan method append\n",
    "s2_append_s1 = s2.append(s1)\n",
    "print(\"Series - append:\\n\", s2_append_s1)\n",
    "# Buat dataframe df1 dan df2\n",
    "df1 = pd.DataFrame({'a':[1,2],\n",
    "\t\t   \t\t\t'b':[3,4]})\n",
    "df2 = pd.DataFrame({'b':[1,2],\n",
    "\t\t   \t\t\t'a':[3,4]})\n",
    "# Terapkan method append\n",
    "df2_append_df1 = df2.append(df1)\n",
    "print(\"Dataframe - append:\\n\", df2_append_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row-wise - concat:\n",
      "    b  a\n",
      "0  1  3\n",
      "1  2  4\n",
      "0  3  1\n",
      "1  4  2\n",
      "Column-wise - concat:\n",
      "    b  a  a  b\n",
      "0  1  3  1  3\n",
      "1  2  4  2  4\n",
      "Multiindex - concat:\n",
      "        b  a\n",
      "df2 0  1  3\n",
      "    1  2  4\n",
      "df1 0  3  1\n",
      "    1  4  2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Buat dataframe df1 dan df2\n",
    "df1 = pd.DataFrame({'a':[1,2],\n",
    "\t\t\t\t\t'b':[3,4]})\n",
    "df2 = pd.DataFrame({'b':[1,2],\n",
    "\t\t\t\t\t'a':[3,4]})\n",
    "# Terapkan method concat row-wise\n",
    "row_wise_concat = pd.concat([df2, df1])\n",
    "print(\"Row-wise - concat:\\n\", row_wise_concat)\n",
    "# Terapkan method concat column-wise\n",
    "col_wise_concat = pd.concat([df2, df1], axis=1)\n",
    "print(\"Column-wise - concat:\\n\", col_wise_concat)\n",
    "# Penambahan identifier --> membentuk hasil penggabungan multiindex\n",
    "multiindex_concat = pd.concat([df2,df1], axis=0, keys=['df2','df1'])\n",
    "print(\"Multiindex - concat:\\n\", multiindex_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge - Left:\n",
      "    key  val3  val4   val1   val2\n",
      "0   k1     1     6  200.0   30.0\n",
      "1   k3     2     7    0.0  100.0\n",
      "2   k5     3     8  100.0   10.0\n",
      "3   k7     4     8    NaN    NaN\n",
      "4  k10     5    10    NaN    NaN\n",
      "Merge - Right:\n",
      "   key  val3  val4  val1  val2\n",
      "0  k1   1.0   6.0   200    30\n",
      "1  k2   NaN   NaN   500    50\n",
      "2  k3   2.0   7.0     0   100\n",
      "3  k4   NaN   NaN   500    20\n",
      "4  k5   3.0   8.0   100    10\n",
      "Merge - Inner:\n",
      "   key  val3  val4  val1  val2\n",
      "0  k1     1     6   200    30\n",
      "1  k3     2     7     0   100\n",
      "2  k5     3     8   100    10\n",
      "Merge - Outer:\n",
      "    key  val3  val4   val1   val2\n",
      "0   k1   1.0   6.0  200.0   30.0\n",
      "1   k3   2.0   7.0    0.0  100.0\n",
      "2   k5   3.0   8.0  100.0   10.0\n",
      "3   k7   4.0   8.0    NaN    NaN\n",
      "4  k10   5.0  10.0    NaN    NaN\n",
      "5   k2   NaN   NaN  500.0   50.0\n",
      "6   k4   NaN   NaN  500.0   20.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Buat dataframe df1 dan df2\n",
    "df1 = pd.DataFrame({\n",
    "   'key':['k1','k2','k3','k4','k5'],\n",
    "   'val1':[200, 500, 0, 500, 100],\n",
    "   'val2':[30, 50, 100, 20, 10]\n",
    "})\n",
    "df2 = pd.DataFrame({\n",
    "   'key':['k1','k3','k5','k7','k10'],\n",
    "   'val3':[1,2,3,4,5],\n",
    "   'val4':[6,7,8,8,10]\n",
    "})\n",
    "# Merge yang ekivalen dengan SQL left join\n",
    "merge_df_left = pd.merge(left=df2, right=df1, how='left', left_on='key', right_on='key')\n",
    "print('Merge - Left:\\n', merge_df_left)\n",
    "# Merge yang ekivalen dengan SQL right join\n",
    "merge_df_right = pd.merge(left=df2, right=df1, how='right', left_on='key', right_on='key')\n",
    "print('Merge - Right:\\n', merge_df_right)\n",
    "# Merge yang ekivalen dengan SQL inner join\n",
    "merge_df_inner = pd.merge(left=df2, right=df1, how='inner', left_on='key', right_on='key')\n",
    "print('Merge - Inner:\\n', merge_df_inner)\n",
    "# Merge yang ekivalen dengan SQL outer join\n",
    "merge_df_outer = pd.merge(left=df2, right=df1, how='outer', left_on='key', right_on='key')\n",
    "print('Merge - Outer:\\n', merge_df_outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge - Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe 1:\n",
      "           val1\n",
      "key val2      \n",
      "k1  30     200\n",
      "k2  50     500\n",
      "k3  100      0\n",
      "k4  20     500\n",
      "k5  10     100\n",
      "Dataframe 2:\n",
      "           val4\n",
      "key val3      \n",
      "k1  1        6\n",
      "k3  2        7\n",
      "k5  3        8\n",
      "k7  4        8\n",
      "k10 5       10\n",
      "Merging dataframe:\n",
      "   key  val2  val1  val3  val4\n",
      "0  k1    30   200     1     6\n",
      "1  k3   100     0     2     7\n",
      "2  k5    10   100     3     8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Buat dataframe df1 dan df2\n",
    "df1 = pd.DataFrame({\n",
    "   'key':['k1','k2','k3','k4','k5'],\n",
    "   'val1':[200, 500, 0, 500, 100],\n",
    "   'val2':[30, 50, 100, 20, 10]\n",
    "}).set_index(['key','val2'])\n",
    "print('Dataframe 1:\\n', df1)\n",
    "df2 = pd.DataFrame({\n",
    "   'key':['k1','k3','k5','k7','k10'],\n",
    "   'val3':[1,2,3,4,5],\n",
    "   'val4':[6,7,8,8,10]\n",
    "}).set_index(['key','val3'])\n",
    "print('Dataframe 2:\\n', df2)\n",
    "# Merge dataframe yang memiliki multi index\n",
    "df_merge = pd.merge(df1.reset_index(),df2.reset_index())\n",
    "print('Merging dataframe:\\n', df_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      val1   val2  val3  val4\n",
      "key                          \n",
      "k1   200.0   30.0   1.0   6.0\n",
      "k10    NaN    NaN   5.0  10.0\n",
      "k2   500.0   50.0   NaN   NaN\n",
      "k3     0.0  100.0   2.0   7.0\n",
      "k4   500.0   20.0   NaN   NaN\n",
      "k5   100.0   10.0   3.0   8.0\n",
      "k7     NaN    NaN   4.0   8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Buat dataframe df1 dan df2\n",
    "df1 = pd.DataFrame({\n",
    "   'key':['k1','k2','k3','k4','k5'],\n",
    "   'val1':[200, 500, 0, 500, 100],\n",
    "   'val2':[30, 50, 100, 20, 10]\n",
    "})\n",
    "df2 = pd.DataFrame({\n",
    "   'key':['k1','k3','k5','k7','k10'],\n",
    "   'val3':[1,2,3,4,5],\n",
    "   'val4':[6,7,8,8,10]\n",
    "})\n",
    "# Penerapan join dengan menggunakan set_index dan keyword how\n",
    "join_df = df1.set_index('key').join(df2.set_index('key'), how='outer')\n",
    "print(join_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique value kelas: ['A' 'B']\n",
      "Unique value murid: ['A1' 'A2' 'A3' 'B1' 'B2' 'B3']\n",
      "Unique value pelajaran: ['math' 'english']\n",
      "Unique value nilai: [ 90  60  70  85  50 100  40  95  80  45]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Dataframe\n",
    "data = pd.DataFrame({\n",
    "  'kelas': 6*['A'] + 6*['B'],\n",
    "  'murid': 2*['A1'] + 2*['A2'] + 2*['A3'] + 2*['B1'] + 2*['B2'] + 2*['B3'],\n",
    "  'pelajaran': 6*['math','english'],\n",
    "  'nilai': [90,60,70,85,50,60,100,40,95,80,60,45]\n",
    "}, columns=['kelas','murid','pelajaran','nilai'])\n",
    "# Unique value pada setiap kolom data\n",
    "for column in data.columns:\n",
    "\tprint('Unique value %s: %s' % (column, data[column].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivoting with single column measurement:\n",
      " pelajaran  english  math\n",
      "murid                   \n",
      "A1              60    90\n",
      "A2              85    70\n",
      "A3              60    50\n",
      "B1              40   100\n",
      "B2              80    95\n",
      "B3              45    60\n",
      "Pivoting with multiple column measurement:\n",
      "             kelas        nilai     \n",
      "pelajaran english math english math\n",
      "murid                              \n",
      "A1              A    A      60   90\n",
      "A2              A    A      85   70\n",
      "A3              A    A      60   50\n",
      "B1              B    B      40  100\n",
      "B2              B    B      80   95\n",
      "B3              B    B      45   60\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Dataframe\n",
    "data = pd.DataFrame({\n",
    "  'kelas': 6*['A'] + 6*['B'],\n",
    "  'murid': 2*['A1'] + 2*['A2'] + 2*['A3'] + 2*['B1'] + 2*['B2'] + 2*['B3'],\n",
    "  'pelajaran': 6*['math','english'],\n",
    "  'nilai': [90,60,70,85,50,60,100,40,95,80,60,45]\n",
    "}, columns=['kelas','murid','pelajaran','nilai'])\n",
    "# Pivoting with single column measurement\n",
    "pivot1 = data.pivot(index='murid', columns='pelajaran', values='nilai')\n",
    "print('Pivoting with single column measurement:\\n', pivot1)\n",
    "# Pivoting with multiple column measurement\n",
    "pivot2 = data.pivot(index='murid', columns='pelajaran')\n",
    "print('Pivoting with multiple column measurement:\\n', pivot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pivot table -- aggfunc mean:\n",
      " pelajaran    english  math\n",
      "kelas                     \n",
      "A          68.333333  70.0\n",
      "B          55.000000  85.0\n",
      "Creating pivot table -- aggfunc median:\n",
      " pelajaran  english  math\n",
      "kelas                   \n",
      "A               60    70\n",
      "B               45    95\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Dataframe\n",
    "data = pd.DataFrame({\n",
    "  'kelas': 6*['A'] + 6*['B'],\n",
    "  'murid': 2*['A1'] + 2*['A2'] + 2*['A3'] + 2*['B1'] + 2*['B2'] + 2*['B3'],\n",
    "  'pelajaran': 6*['math','english'],\n",
    "  'nilai': [90,60,70,85,50,60,100,40,95,80,60,45]\n",
    "}, columns=['kelas','murid','pelajaran','nilai'])\n",
    "# Creating pivot and assign pivot_tab dengan menggunakan keyword aggfunc='mean'\n",
    "pivot_tab_mean = data.pivot_table(index='kelas',columns='pelajaran',values='nilai', aggfunc='mean')\n",
    "print('Creating pivot table -- aggfunc mean:\\n', pivot_tab_mean)\n",
    "# Creating pivot and assign pivot_tab dengan menggunakan keyword aggfunc='median'\n",
    "pivot_tab_median = data.pivot_table(index='kelas',columns='pelajaran',values='nilai', aggfunc='median')\n",
    "print('Creating pivot table -- aggfunc median:\\n', pivot_tab_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melt - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivoting dataframe:\n",
      " pelajaran kelas    english  math\n",
      "0             A  68.333333  70.0\n",
      "1             B  55.000000  85.0\n",
      "Melting dataframe:\n",
      "   pelajaran      value\n",
      "0     kelas          A\n",
      "1     kelas          B\n",
      "2   english  68.333333\n",
      "3   english  55.000000\n",
      "4      math  70.000000\n",
      "5      math  85.000000\n",
      "Melting dataframe dengan idvars:\n",
      "   kelas pelajaran      value\n",
      "0     A   english  68.333333\n",
      "1     B   english  55.000000\n",
      "2     A      math  70.000000\n",
      "3     B      math  85.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Dataframe\n",
    "data = pd.DataFrame({\n",
    "  'kelas': 6*['A'] + 6*['B'],\n",
    "  'murid': 2*['A1'] + 2*['A2'] + 2*['A3'] + 2*['B1'] + 2*['B2'] + 2*['B3'],\n",
    "  'pelajaran': 6*['math','english'],\n",
    "  'nilai': [90,60,70,85,50,60,100,40,95,80,60,45]\n",
    "}, columns=['kelas','murid','pelajaran','nilai'])\n",
    "# Pivoting dataframe\n",
    "data_pivot = data.pivot_table(index='kelas',columns='pelajaran',values='nilai',aggfunc='mean').reset_index()\n",
    "print('Pivoting dataframe:\\n', data_pivot)\n",
    "# [1] Melting dataframe data_pivot\n",
    "data_melt_1 = pd.melt(data_pivot)\n",
    "print('Melting dataframe:\\n', data_melt_1)\n",
    "# [2] Melting dataframe data_pivot dengan id_vars\n",
    "data_melt_2 = pd.melt(data_pivot, id_vars='kelas')\n",
    "print('Melting dataframe dengan idvars:\\n', data_melt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melt - Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivoting dataframe:\n",
      " pelajaran kelas    english  math\n",
      "0             A  68.333333  70.0\n",
      "1             B  55.000000  85.0\n",
      "Melting dataframe dengan value_vars:\n",
      "   pelajaran  value\n",
      "0      math   70.0\n",
      "1      math   85.0\n",
      "Melting dataframe dengan id_vars dan value_vars:\n",
      "   kelas pelajaran  value\n",
      "0     A      math   70.0\n",
      "1     B      math   85.0\n",
      "Melting dataframe dengan id_vars, value_vars, var_name. dan value_name:\n",
      "   kelas pelajaran      nilai\n",
      "0     A   english  68.333333\n",
      "1     B   english  55.000000\n",
      "2     A      math  70.000000\n",
      "3     B      math  85.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Dataframe\n",
    "data = pd.DataFrame({\n",
    "  'kelas': 6*['A'] + 6*['B'],\n",
    "  'murid': 2*['A1'] + 2*['A2'] + 2*['A3'] + 2*['B1'] + 2*['B2'] + 2*['B3'],\n",
    "  'pelajaran': 6*['math','english'],\n",
    "  'nilai': [90,60,70,85,50,60,100,40,95,80,60,45]\n",
    "}, columns=['kelas','murid','pelajaran','nilai'])\n",
    "# Pivoting dataframe\n",
    "data_pivot = data.pivot_table(index='kelas',columns='pelajaran',values='nilai', aggfunc='mean').reset_index()\n",
    "print('Pivoting dataframe:\\n', data_pivot)\n",
    "# [3.a] Melting dataframe data_pivot dengan value_vars\n",
    "data_melt_3a = pd.melt(data_pivot, value_vars=['math'])\n",
    "print('Melting dataframe dengan value_vars:\\n', data_melt_3a)\n",
    "# [3.b] Melting dataframe data_pivot dengan id_vars dan value_vars\n",
    "data_melt_3b = pd.melt(data_pivot, id_vars='kelas', value_vars=['math'])\n",
    "print('Melting dataframe dengan id_vars dan value_vars:\\n', data_melt_3b)\n",
    "# [4] Melting dataframe data_pivot dengan id_vars, value_vars, var_name. dan value_name\n",
    "data_melt_4 = pd.melt(data_pivot, id_vars='kelas', value_vars=['english','math'], var_name='pelajaran', value_name='nilai')\n",
    "print('Melting dataframe dengan id_vars, value_vars, var_name. dan value_name:\\n', data_melt_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack & Unstack - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe:\n",
      "    kelas murid pelajaran  nilai\n",
      "0      A    A1      math     90\n",
      "1      A    A1   english     60\n",
      "2      A    A2      math     70\n",
      "3      A    A2   english     85\n",
      "4      A    A3      math     50\n",
      "5      A    A3   english     60\n",
      "6      B    B1      math    100\n",
      "7      B    B1   english     40\n",
      "8      B    B2      math     95\n",
      "9      B    B2   english     80\n",
      "10     B    B3      math     60\n",
      "11     B    B3   english     45\n",
      "Dataframe multi index:\n",
      "                        nilai\n",
      "kelas murid pelajaran       \n",
      "A     A1    math          90\n",
      "            english       60\n",
      "      A2    math          70\n",
      "            english       85\n",
      "      A3    math          50\n",
      "            english       60\n",
      "B     B1    math         100\n",
      "            english       40\n",
      "      B2    math          95\n",
      "            english       80\n",
      "      B3    math          60\n",
      "            english       45\n",
      "Unstacking dataframe:\n",
      "               nilai     \n",
      "pelajaran   english math\n",
      "kelas murid             \n",
      "A     A1         60   90\n",
      "      A2         85   70\n",
      "      A3         60   50\n",
      "B     B1         40  100\n",
      "      B2         80   95\n",
      "      B3         45   60\n",
      "Unstacking dataframe dengan level name:\n",
      "                 nilai                               \n",
      "murid              A1    A2    A3     B1    B2    B3\n",
      "kelas pelajaran                                     \n",
      "A     english    60.0  85.0  60.0    NaN   NaN   NaN\n",
      "      math       90.0  70.0  50.0    NaN   NaN   NaN\n",
      "B     english     NaN   NaN   NaN   40.0  80.0  45.0\n",
      "      math        NaN   NaN   NaN  100.0  95.0  60.0\n",
      "Unstacking dataframe dengan level position:\n",
      "                 nilai                               \n",
      "murid              A1    A2    A3     B1    B2    B3\n",
      "kelas pelajaran                                     \n",
      "A     english    60.0  85.0  60.0    NaN   NaN   NaN\n",
      "      math       90.0  70.0  50.0    NaN   NaN   NaN\n",
      "B     english     NaN   NaN   NaN   40.0  80.0  45.0\n",
      "      math        NaN   NaN   NaN  100.0  95.0  60.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Dataframe\n",
    "data = pd.DataFrame({\n",
    "  'kelas': 6*['A'] + 6*['B'],\n",
    "  'murid': 2*['A1'] + 2*['A2'] + 2*['A3'] + 2*['B1'] + 2*['B2'] + 2*['B3'],\n",
    "  'pelajaran': 6*['math','english'],\n",
    "  'nilai': [90,60,70,85,50,60,100,40,95,80,60,45]\n",
    "}, columns=['kelas','murid','pelajaran','nilai'])\n",
    "print('Dataframe:\\n', data)\n",
    "# Set index data untuk kolom kelas, murid, dan pelajaran\n",
    "data = data.set_index(['kelas','murid','pelajaran'])\n",
    "print('Dataframe multi index:\\n', data)\n",
    "# [1] Unstacking dataframe\n",
    "data_unstack_1 = data.unstack()\n",
    "print('Unstacking dataframe:\\n', data_unstack_1)\n",
    "# [2] Unstacking dengan specify level name\n",
    "data_unstack_2 = data.unstack(level='murid')\n",
    "print('Unstacking dataframe dengan level name:\\n', data_unstack_2)\n",
    "# [3] Unstacking dengan specify level position\n",
    "data_unstack_3 = data.unstack(level=1)\n",
    "print('Unstacking dataframe dengan level position:\\n', data_unstack_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack & Unstack - Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe:\n",
      "                 nilai                               \n",
      "murid              A1    A2    A3     B1    B2    B3\n",
      "kelas pelajaran                                     \n",
      "A     english    60.0  85.0  60.0    NaN   NaN   NaN\n",
      "      math       90.0  70.0  50.0    NaN   NaN   NaN\n",
      "B     english     NaN   NaN   NaN   40.0  80.0  45.0\n",
      "      math        NaN   NaN   NaN  100.0  95.0  60.0\n",
      "Stacked dataframe:\n",
      "                        nilai\n",
      "kelas pelajaran murid       \n",
      "A     english   A1      60.0\n",
      "                A2      85.0\n",
      "                A3      60.0\n",
      "      math      A1      90.0\n",
      "                A2      70.0\n",
      "                A3      50.0\n",
      "B     english   B1      40.0\n",
      "                B2      80.0\n",
      "                B3      45.0\n",
      "      math      B1     100.0\n",
      "                B2      95.0\n",
      "                B3      60.0\n",
      "Swapped data:\n",
      "                        nilai\n",
      "kelas murid pelajaran       \n",
      "A     A1    english     60.0\n",
      "      A2    english     85.0\n",
      "      A3    english     60.0\n",
      "      A1    math        90.0\n",
      "      A2    math        70.0\n",
      "      A3    math        50.0\n",
      "B     B1    english     40.0\n",
      "      B2    english     80.0\n",
      "      B3    english     45.0\n",
      "      B1    math       100.0\n",
      "      B2    math        95.0\n",
      "      B3    math        60.0\n",
      "Sorted data:\n",
      "                        nilai\n",
      "kelas murid pelajaran       \n",
      "A     A1    english     60.0\n",
      "            math        90.0\n",
      "      A2    english     85.0\n",
      "            math        70.0\n",
      "      A3    english     60.0\n",
      "            math        50.0\n",
      "B     B1    english     40.0\n",
      "            math       100.0\n",
      "      B2    english     80.0\n",
      "            math        95.0\n",
      "      B3    english     45.0\n",
      "            math        60.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Dataframe\n",
    "data = pd.DataFrame({\n",
    "  'kelas': 6*['A'] + 6*['B'],\n",
    "  'murid': 2*['A1'] + 2*['A2'] + 2*['A3'] + 2*['B1'] + 2*['B2'] + 2*['B3'],\n",
    "  'pelajaran': 6*['math','english'],\n",
    "  'nilai': [90,60,70,85,50,60,100,40,95,80,60,45]\n",
    "}, columns=['kelas','murid','pelajaran','nilai'])\n",
    "data = data.set_index(['kelas','murid','pelajaran'])\n",
    "data_unstack = data.unstack(level=1)\n",
    "print('Dataframe:\\n', data_unstack)\n",
    "# [1] Stacking dataframe\n",
    "data_stack = data_unstack.stack()\n",
    "print('Stacked dataframe:\\n', data_stack)\n",
    "# [2] Tukar posisi index setelah stacking dataframe\n",
    "data_swap = data_stack.swaplevel(1,2)\n",
    "print('Swapped data:\\n', data_swap)\n",
    "# [3] Melakukan sort_index pada stacking dataframe\n",
    "data_sort = data_swap.sort_index()\n",
    "print('Sorted data:\\n', data_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Inspeksi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lima data teratas:\n",
      "                           location       city country pollutant  value  \\\n",
      "0                  MOBILE-KICKAPOO    LINCOLN      US      pm10   7.00   \n",
      "1                  Oxford St Ebbes     Oxford      GB       no2  30.00   \n",
      "2                 BROADWAY (South)  St. Louis      US      pm25   6.10   \n",
      "3  Deen Dayal Nagar, Sagar - MPPCB      Sagar      IN      pm25  23.67   \n",
      "4                        Manglerud       Oslo      NO      pm10  27.06   \n",
      "\n",
      "                 timestamp   unit source_name   latitude  longitude  \\\n",
      "0  2017-01-18 16:00:00 UTC  µg/m³      AirNow  35.488400 -97.090280   \n",
      "1  2020-04-07 20:00:00 UTC  µg/m³       DEFRA  51.744804  -1.260278   \n",
      "2  2020-04-07 19:00:00 UTC  µg/m³      AirNow  38.542500 -90.263610   \n",
      "3  2020-04-07 18:30:00 UTC  µg/m³       caaqm  23.864016  78.802895   \n",
      "4  2020-04-07 20:00:00 UTC  µg/m³      Norway  59.898690  10.814950   \n",
      "\n",
      "   averaged_over_in_hours  \n",
      "0                    1.00  \n",
      "1                    1.00  \n",
      "2                    1.00  \n",
      "3                    0.25  \n",
      "4                    1.00  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   location                3997 non-null   object \n",
      " 1   city                    3966 non-null   object \n",
      " 2   country                 4000 non-null   object \n",
      " 3   pollutant               4000 non-null   object \n",
      " 4   value                   4000 non-null   float64\n",
      " 5   timestamp               4000 non-null   object \n",
      " 6   unit                    4000 non-null   object \n",
      " 7   source_name             4000 non-null   object \n",
      " 8   latitude                4000 non-null   float64\n",
      " 9   longitude               4000 non-null   float64\n",
      " 10  averaged_over_in_hours  3634 non-null   float64\n",
      "dtypes: float64(4), object(7)\n",
      "memory usage: 343.9+ KB\n",
      "Info global_air_quality:\n",
      " None\n",
      "Count tanpa groupby:\n",
      " location                  3997\n",
      "city                      3966\n",
      "country                   4000\n",
      "pollutant                 4000\n",
      "value                     4000\n",
      "timestamp                 4000\n",
      "unit                      4000\n",
      "source_name               4000\n",
      "latitude                  4000\n",
      "longitude                 4000\n",
      "averaged_over_in_hours    3634\n",
      "dtype: int64\n",
      "Count dengan groupby (5 data teratas):\n",
      "              location  city  country  pollutant  value  timestamp  unit  \\\n",
      "source_name                                                               \n",
      "ARPALAZIO          72    72       72         72     72         72    72   \n",
      "Agaar.mn           27    27       27         27     27         27    27   \n",
      "AirNow           1712  1681     1715       1715   1715       1715  1715   \n",
      "Andalucia          71    71       71         71     71         71    71   \n",
      "Anqing              4     4        4          4      4          4     4   \n",
      "\n",
      "             latitude  longitude  averaged_over_in_hours  \n",
      "source_name                                               \n",
      "ARPALAZIO          72         72                      72  \n",
      "Agaar.mn           27         27                       0  \n",
      "AirNow           1715       1715                    1715  \n",
      "Andalucia          71         71                      71  \n",
      "Anqing              4          4                       4  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load data global_air_quality.csv\n",
    "global_air_quality = pd.read_csv('https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv')\n",
    "print('Lima data teratas:\\n', global_air_quality.head())\n",
    "# Melakukan pengecekan terhadap data\n",
    "print('Info global_air_quality:\\n', global_air_quality.info())\n",
    "# Melakukan count tanpa groupby\n",
    "print('Count tanpa groupby:\\n', global_air_quality.count())\n",
    "# Melakukan count dengan groupby \n",
    "gaq_groupby_count = global_air_quality.groupby('source_name').count()\n",
    "print('Count dengan groupby (5 data teratas):\\n', gaq_groupby_count.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby dan Aggregasi dengan Fungsi Statistik Dasar - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pollutant (5 teratas):\n",
      "                      value                                     \n",
      "pollutant               bc      co   no2   o3   pm10  pm25  so2\n",
      "country city                                                   \n",
      "AR      Buenos Aires   0.0     0.0   0.0  0.0    0.0  18.1  0.0\n",
      "AU      Townsville     0.0     0.0   0.0  0.0    0.0   3.9  0.0\n",
      "BA      Goražde        0.0   141.0  19.0  8.0    0.0   0.0  0.0\n",
      "        Ilijaš         0.0     0.0   0.0  0.0  100.0   0.0  0.0\n",
      "        Jajce          0.0  1508.0  25.0  6.0    9.0   0.0  0.0\n",
      "Rata-rata pollutant (5 teratas):\n",
      "           value                                                         \\\n",
      "pollutant    bc          co        no2        o3       pm10       pm25   \n",
      "country                                                                  \n",
      "AR          0.0    0.000000   0.000000  0.000000   0.000000  18.100000   \n",
      "AU          0.0    0.000000   0.000000  0.000000   0.000000   3.900000   \n",
      "BA          0.0  475.833333  19.500000  5.833333  40.333333   0.000000   \n",
      "CA          0.0    0.036818   0.000355  0.025963   0.836364   3.433601   \n",
      "CL          0.0    0.000000  21.000000  0.000000   0.000000   0.000000   \n",
      "\n",
      "                      \n",
      "pollutant        so2  \n",
      "country               \n",
      "AR          0.000000  \n",
      "AU          0.000000  \n",
      "BA         18.000000  \n",
      "CA          0.000091  \n",
      "CL          0.000000  \n",
      "Standar deviasi pollutant (5 teratas):\n",
      "           value                                                        \\\n",
      "pollutant    bc          co        no2        o3       pm10      pm25   \n",
      "country                                                                 \n",
      "AR          0.0    0.000000   0.000000  0.000000   0.000000  0.000000   \n",
      "AU          0.0    0.000000   0.000000  0.000000   0.000000  0.000000   \n",
      "BA          0.0  536.925476  11.945711  8.207720  46.701891  0.000000   \n",
      "CA          0.0    0.089841   0.000750  0.019323   1.713052  2.286509   \n",
      "CL          0.0    0.000000   0.000000  0.000000   0.000000  0.000000   \n",
      "\n",
      "                      \n",
      "pollutant        so2  \n",
      "country               \n",
      "AR          0.000000  \n",
      "AU          0.000000  \n",
      "BA         30.403947  \n",
      "CA          0.000302  \n",
      "CL          0.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load data global_air_quality.csv\n",
    "gaq = pd.read_csv('https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv')\n",
    "# Create variabel pollutant\n",
    "pollutant = gaq[['country','city','pollutant','value']].pivot_table(index=['country','city'],columns='pollutant').fillna(0)\n",
    "print('Data pollutant (5 teratas):\\n', pollutant.head())\n",
    "# [1] Group berdasarkan country dan terapkan aggregasi mean\n",
    "pollutant_mean = pollutant.groupby('country').mean()\n",
    "print('Rata-rata pollutant (5 teratas):\\n', pollutant_mean.head())\n",
    "# [2] Group berdasarkan country dan terapkan aggregasi std\n",
    "pollutant_std = pollutant.groupby('country').std().fillna(0)\n",
    "print('Standar deviasi pollutant (5 teratas):\\n', pollutant_std.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby dan Aggregasi dengan Fungsi Statistik Dasar - Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pollutant (5 teratas):\n",
      "                      value                                     \n",
      "pollutant               bc      co   no2   o3   pm10  pm25  so2\n",
      "country city                                                   \n",
      "AR      Buenos Aires   0.0     0.0   0.0  0.0    0.0  18.1  0.0\n",
      "AU      Townsville     0.0     0.0   0.0  0.0    0.0   3.9  0.0\n",
      "BA      Goražde        0.0   141.0  19.0  8.0    0.0   0.0  0.0\n",
      "        Ilijaš         0.0     0.0   0.0  0.0  100.0   0.0  0.0\n",
      "        Jajce          0.0  1508.0  25.0  6.0    9.0   0.0  0.0\n",
      "Total pollutant (5 teratas):\n",
      "           value                                                          \n",
      "pollutant    bc        co       no2         o3   pm10       pm25      so2\n",
      "country                                                                  \n",
      "AR          0.0     0.000    0.0000   0.000000    0.0  18.100000    0.000\n",
      "AU          0.0     0.000    0.0000   0.000000    0.0   3.900000    0.000\n",
      "BA          0.0  2855.000  117.0000  35.000000  242.0   0.000000  108.000\n",
      "CA          0.0     0.405    0.0039   0.285593    9.2  37.769608    0.001\n",
      "CL          0.0     0.000   21.0000   0.000000    0.0   0.000000    0.000\n",
      "Jumlah unique value pollutant (5 teratas):\n",
      "           value                        \n",
      "pollutant    bc co no2 o3 pm10 pm25 so2\n",
      "country                                \n",
      "AR            1  1   1  1    1    1   1\n",
      "AU            1  1   1  1    1    1   1\n",
      "BA            1  6   6  4    5    1   4\n",
      "CA            1  3   4  9    4   10   2\n",
      "CL            1  1   1  1    1    1   1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load data https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv\n",
    "gaq = pd.read_csv('https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv')\n",
    "# Create variabel pollutant \n",
    "pollutant = gaq[['country','city','pollutant','value']].pivot_table(index=['country','city'],columns='pollutant').fillna(0)\n",
    "print('Data pollutant (5 teratas):\\n', pollutant.head())\n",
    "# [3] Group berdasarkan country dan terapkan aggregasi sum\n",
    "pollutant_sum = pollutant.groupby('country').sum()\n",
    "print('Total pollutant (5 teratas):\\n', pollutant_sum.head())\n",
    "# [4] Group berdasarkan country dan terapkan aggregasi nunique\n",
    "pollutant_nunique = pollutant.groupby('country').nunique()\n",
    "print('Jumlah unique value pollutant (5 teratas):\\n', pollutant_nunique.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby dan Aggregasi dengan Fungsi Statistik Dasar - Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pollutant (5 teratas):\n",
      "                      value                                     \n",
      "pollutant               bc      co   no2   o3   pm10  pm25  so2\n",
      "country city                                                   \n",
      "AR      Buenos Aires   0.0     0.0   0.0  0.0    0.0  18.1  0.0\n",
      "AU      Townsville     0.0     0.0   0.0  0.0    0.0   3.9  0.0\n",
      "BA      Goražde        0.0   141.0  19.0  8.0    0.0   0.0  0.0\n",
      "        Ilijaš         0.0     0.0   0.0  0.0  100.0   0.0  0.0\n",
      "        Jajce          0.0  1508.0  25.0  6.0    9.0   0.0  0.0\n",
      "Item pertama pollutant (5 teratas):\n",
      "           value                                            \n",
      "pollutant    bc       co      no2        o3 pm10  pm25  so2\n",
      "country                                                    \n",
      "AR          0.0    0.000   0.0000  0.000000  0.0  18.1  0.0\n",
      "AU          0.0    0.000   0.0000  0.000000  0.0   3.9  0.0\n",
      "BA          0.0  141.000  19.0000  8.000000  0.0   0.0  0.0\n",
      "CA          0.0    0.285   0.0024  0.048333  0.0   3.9  0.0\n",
      "CL          0.0    0.000  21.0000  0.000000  0.0   0.0  0.0\n",
      "Item terakhir pollutant (5 teratas):\n",
      "           value                                      \n",
      "pollutant    bc     co   no2     o3  pm10  pm25   so2\n",
      "country                                              \n",
      "AR          0.0    0.0   0.0  0.000   0.0  18.1   0.0\n",
      "AU          0.0    0.0   0.0  0.000   0.0   3.9   0.0\n",
      "BA          0.0  292.0  29.0  0.000  96.0   0.0  78.0\n",
      "CA          0.0    0.0   0.0  0.036   3.0   0.0   0.0\n",
      "CL          0.0    0.0  21.0  0.000   0.0   0.0   0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load data https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv\n",
    "gaq = pd.read_csv('https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv')\n",
    "# Create variabel pollutant \n",
    "pollutant = gaq[['country','city','pollutant','value']].pivot_table(index=['country','city'],columns='pollutant').fillna(0)\n",
    "print('Data pollutant (5 teratas):\\n', pollutant.head())\n",
    "# Group berdasarkan country dan terapkan aggregasi first\n",
    "pollutant_first = pollutant.groupby('country').first()\n",
    "print('Item pertama pollutant (5 teratas):\\n', pollutant_first.head())\n",
    "# Group berdasarkan country dan terapkan aggregasi last\n",
    "pollutant_last = pollutant.groupby('country').last()\n",
    "print('Item terakhir pollutant (5 teratas):\\n', pollutant_last.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby dengan Multiple Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pollutant (5 teratas):\n",
      "                      value                                     \n",
      "pollutant               bc      co   no2   o3   pm10  pm25  so2\n",
      "country city                                                   \n",
      "AR      Buenos Aires   0.0     0.0   0.0  0.0    0.0  18.1  0.0\n",
      "AU      Townsville     0.0     0.0   0.0  0.0    0.0   3.9  0.0\n",
      "BA      Goražde        0.0   141.0  19.0  8.0    0.0   0.0  0.0\n",
      "        Ilijaš         0.0     0.0   0.0  0.0  100.0   0.0  0.0\n",
      "        Jajce          0.0  1508.0  25.0  6.0    9.0   0.0  0.0\n",
      "Multiple aggregations (5 teratas):\n",
      "         value                                                                  \\\n",
      "           bc                    co                                no2          \n",
      "          min median mean  max  min median        mean       max   min median   \n",
      "country                                                                         \n",
      "AR        0.0    0.0  0.0  0.0  0.0    0.0    0.000000     0.000   0.0    0.0   \n",
      "AU        0.0    0.0  0.0  0.0  0.0    0.0    0.000000     0.000   0.0    0.0   \n",
      "BA        0.0    0.0  0.0  0.0  0.0  355.0  475.833333  1508.000   0.0   22.0   \n",
      "CA        0.0    0.0  0.0  0.0  0.0    0.0    0.036818     0.285   0.0    0.0   \n",
      "CL        0.0    0.0  0.0  0.0  0.0    0.0    0.000000     0.000  21.0   21.0   \n",
      "\n",
      "         ...                                                              \\\n",
      "         ...       pm10         pm25                          so2          \n",
      "         ...       mean    max   min median       mean   max  min median   \n",
      "country  ...                                                               \n",
      "AR       ...   0.000000    0.0  18.1   18.1  18.100000  18.1  0.0    0.0   \n",
      "AU       ...   0.000000    0.0   3.9    3.9   3.900000   3.9  0.0    0.0   \n",
      "BA       ...  40.333333  100.0   0.0    0.0   0.000000   0.0  0.0    5.5   \n",
      "CA       ...   0.836364    5.2   0.0    4.0   3.433601   6.0  0.0    0.0   \n",
      "CL       ...   0.000000    0.0   0.0    0.0   0.000000   0.0  0.0    0.0   \n",
      "\n",
      "                            \n",
      "                            \n",
      "              mean     max  \n",
      "country                     \n",
      "AR        0.000000   0.000  \n",
      "AU        0.000000   0.000  \n",
      "BA       18.000000  78.000  \n",
      "CA        0.000091   0.001  \n",
      "CL        0.000000   0.000  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load data https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv\n",
    "gaq = pd.read_csv('https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv')\n",
    "# Create variabel pollutant\n",
    "pollutant = gaq[['country','city','pollutant','value']].pivot_table(index=['country','city'],columns='pollutant').fillna(0)\n",
    "print('Data pollutant (5 teratas):\\n', pollutant.head())\n",
    "# Group berdasarkan country dan terapkan aggregasi: min, median, mean, max\n",
    "multiagg = pollutant.groupby('country').agg(['min','median','mean','max'])\n",
    "print('Multiple aggregations (5 teratas):\\n', multiagg.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby dengan Custom Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom aggregation (5 teratas):\n",
      "         value                                                  \n",
      "           bc      co       no2        o3  pm10      pm25   so2\n",
      "country                                                        \n",
      "AR        0.0    0.00   0.00000  0.000000   0.0  0.000000   0.0\n",
      "AU        0.0    0.00   0.00000  0.000000   0.0  0.000000   0.0\n",
      "BA        0.0  297.75  14.25000  7.500000  79.0  0.000000  17.0\n",
      "CA        0.0    0.00   0.00025  0.035708   0.5  3.876471   0.0\n",
      "CL        0.0    0.00   0.00000  0.000000   0.0  0.000000   0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load data https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv\n",
    "gaq = pd.read_csv('https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv')\n",
    "# Create variabel pollutant \n",
    "pollutant = gaq[['country','city','pollutant','value']].pivot_table(index=['country','city'],columns='pollutant').fillna(0)\n",
    "# Create sebuah function: iqr\n",
    "def iqr(series):\n",
    "\tQ1 = series.quantile(0.25)\n",
    "\tQ3 = series.quantile(0.75)\n",
    "\treturn Q3 - Q1\n",
    "# Group berdasarkan country dan terapkan aggregasi dari function: iqr\n",
    "custom_agg = pollutant.groupby('country').agg(iqr)\n",
    "print('Custom aggregation (5 teratas):\\n', custom_agg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby dengan Custom Aggregations by dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pollutant (5 teratas):\n",
      "                      value                                     \n",
      "pollutant               bc      co   no2   o3   pm10  pm25  so2\n",
      "country city                                                   \n",
      "AR      Buenos Aires   0.0     0.0   0.0  0.0    0.0  18.1  0.0\n",
      "AU      Townsville     0.0     0.0   0.0  0.0    0.0   3.9  0.0\n",
      "BA      Goražde        0.0   141.0  19.0  8.0    0.0   0.0  0.0\n",
      "        Ilijaš         0.0     0.0   0.0  0.0  100.0   0.0  0.0\n",
      "        Jajce          0.0  1508.0  25.0  6.0    9.0   0.0  0.0\n",
      "\n",
      "Cetak 5 data teratas custom_agg_dict:\n",
      "          pm10      pm25   so2\n",
      "country                      \n",
      "AR        0.0  0.000000   0.0\n",
      "AU        0.0  0.000000   0.0\n",
      "BA       23.0  0.000000  17.0\n",
      "CA        0.0  3.876471   0.0\n",
      "CL        0.0  0.000000   0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load data https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv\n",
    "gaq = pd.read_csv('https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv')\n",
    "# Create variabel pollutant\n",
    "pollutant = gaq[['country','city','pollutant','value']].pivot_table(index=['country','city'],columns='pollutant').fillna(0)\n",
    "print('Data pollutant (5 teratas):\\n', pollutant.head())\n",
    "# Function IQR\n",
    "def iqr(series):\n",
    "\treturn series.quantile(0.75) - series.quantile(0.25)\n",
    "# Create custom aggregation using dict\n",
    "custom_agg_dict = pollutant['value'][['pm10','pm25','so2']].groupby('country').agg({\n",
    "\t'pm10':'median',\n",
    "\t'pm25':iqr,\t\n",
    "\t'so2':iqr\n",
    "})\n",
    "print('\\nCetak 5 data teratas custom_agg_dict:\\n', custom_agg_dict.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset as Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  location       city country  \\\n",
      "timestamp                                                                       \n",
      "2017-01-18 16:00:00+00:00                  MOBILE-KICKAPOO    LINCOLN      US   \n",
      "2020-04-07 20:00:00+00:00                  Oxford St Ebbes     Oxford      GB   \n",
      "2020-04-07 19:00:00+00:00                 BROADWAY (South)  St. Louis      US   \n",
      "2020-04-07 18:30:00+00:00  Deen Dayal Nagar, Sagar - MPPCB      Sagar      IN   \n",
      "2020-04-07 20:00:00+00:00                        Manglerud       Oslo      NO   \n",
      "\n",
      "                          pollutant  value   unit source_name   latitude  \\\n",
      "timestamp                                                                  \n",
      "2017-01-18 16:00:00+00:00      pm10   7.00  µg/m³      AirNow  35.488400   \n",
      "2020-04-07 20:00:00+00:00       no2  30.00  µg/m³       DEFRA  51.744804   \n",
      "2020-04-07 19:00:00+00:00      pm25   6.10  µg/m³      AirNow  38.542500   \n",
      "2020-04-07 18:30:00+00:00      pm25  23.67  µg/m³       caaqm  23.864016   \n",
      "2020-04-07 20:00:00+00:00      pm10  27.06  µg/m³      Norway  59.898690   \n",
      "\n",
      "                           longitude  averaged_over_in_hours  \n",
      "timestamp                                                     \n",
      "2017-01-18 16:00:00+00:00 -97.090280                    1.00  \n",
      "2020-04-07 20:00:00+00:00  -1.260278                    1.00  \n",
      "2020-04-07 19:00:00+00:00 -90.263610                    1.00  \n",
      "2020-04-07 18:30:00+00:00  78.802895                    0.25  \n",
      "2020-04-07 20:00:00+00:00  10.814950                    1.00  \n",
      "info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 4000 entries, 2017-01-18 16:00:00+00:00 to 2017-01-08 08:00:00+00:00\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   location                3997 non-null   object \n",
      " 1   city                    3966 non-null   object \n",
      " 2   country                 4000 non-null   object \n",
      " 3   pollutant               4000 non-null   object \n",
      " 4   value                   4000 non-null   float64\n",
      " 5   unit                    4000 non-null   object \n",
      " 6   source_name             4000 non-null   object \n",
      " 7   latitude                4000 non-null   float64\n",
      " 8   longitude               4000 non-null   float64\n",
      " 9   averaged_over_in_hours  3634 non-null   float64\n",
      "dtypes: float64(4), object(6)\n",
      "memory usage: 343.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load dataset https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv\n",
    "gaq = pd.read_csv('https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv', parse_dates=True, index_col='timestamp')\n",
    "# Cetak 5 data teratas\n",
    "print(gaq.head())\n",
    "# Cetak info dari dataframe gaq\n",
    "print('info')\n",
    "print(gaq.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebelum diubah dalam format datetime:\n",
      "                           location       city country pollutant  value  \\\n",
      "0                  MOBILE-KICKAPOO    LINCOLN      US      pm10   7.00   \n",
      "1                  Oxford St Ebbes     Oxford      GB       no2  30.00   \n",
      "2                 BROADWAY (South)  St. Louis      US      pm25   6.10   \n",
      "3  Deen Dayal Nagar, Sagar - MPPCB      Sagar      IN      pm25  23.67   \n",
      "4                        Manglerud       Oslo      NO      pm10  27.06   \n",
      "\n",
      "                 timestamp   unit source_name   latitude  longitude  \\\n",
      "0  2017-01-18 16:00:00 UTC  µg/m³      AirNow  35.488400 -97.090280   \n",
      "1  2020-04-07 20:00:00 UTC  µg/m³       DEFRA  51.744804  -1.260278   \n",
      "2  2020-04-07 19:00:00 UTC  µg/m³      AirNow  38.542500 -90.263610   \n",
      "3  2020-04-07 18:30:00 UTC  µg/m³       caaqm  23.864016  78.802895   \n",
      "4  2020-04-07 20:00:00 UTC  µg/m³      Norway  59.898690  10.814950   \n",
      "\n",
      "   averaged_over_in_hours  \n",
      "0                    1.00  \n",
      "1                    1.00  \n",
      "2                    1.00  \n",
      "3                    0.25  \n",
      "4                    1.00  \n",
      "Sesudah diubah dalam format datetime:\n",
      "                                                   location       city country  \\\n",
      "timestamp                                                                       \n",
      "2017-01-18 16:00:00+00:00                  MOBILE-KICKAPOO    LINCOLN      US   \n",
      "2020-04-07 20:00:00+00:00                  Oxford St Ebbes     Oxford      GB   \n",
      "2020-04-07 19:00:00+00:00                 BROADWAY (South)  St. Louis      US   \n",
      "2020-04-07 18:30:00+00:00  Deen Dayal Nagar, Sagar - MPPCB      Sagar      IN   \n",
      "2020-04-07 20:00:00+00:00                        Manglerud       Oslo      NO   \n",
      "\n",
      "                          pollutant  value   unit source_name   latitude  \\\n",
      "timestamp                                                                  \n",
      "2017-01-18 16:00:00+00:00      pm10   7.00  µg/m³      AirNow  35.488400   \n",
      "2020-04-07 20:00:00+00:00       no2  30.00  µg/m³       DEFRA  51.744804   \n",
      "2020-04-07 19:00:00+00:00      pm25   6.10  µg/m³      AirNow  38.542500   \n",
      "2020-04-07 18:30:00+00:00      pm25  23.67  µg/m³       caaqm  23.864016   \n",
      "2020-04-07 20:00:00+00:00      pm10  27.06  µg/m³      Norway  59.898690   \n",
      "\n",
      "                           longitude  averaged_over_in_hours  \n",
      "timestamp                                                     \n",
      "2017-01-18 16:00:00+00:00 -97.090280                    1.00  \n",
      "2020-04-07 20:00:00+00:00  -1.260278                    1.00  \n",
      "2020-04-07 19:00:00+00:00 -90.263610                    1.00  \n",
      "2020-04-07 18:30:00+00:00  78.802895                    0.25  \n",
      "2020-04-07 20:00:00+00:00  10.814950                    1.00  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load dataset https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv\n",
    "gaq = pd.read_csv('https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv')\n",
    "# Cetak 5 data teratas\n",
    "print('Sebelum diubah dalam format datetime:\\n', gaq.head())\n",
    "# Ubah menjadi datetime\n",
    "gaq['timestamp'] = pd.to_datetime(gaq['timestamp'])\n",
    "gaq = gaq.set_index('timestamp')\n",
    "# Cetak 5 data teratas\n",
    "print('Sesudah diubah dalam format datetime:\\n', gaq.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsampling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sebelum di-downsampling (5 teratas):\n",
      "                                                   location       city country  \\\n",
      "timestamp                                                                       \n",
      "2017-01-18 16:00:00+00:00                  MOBILE-KICKAPOO    LINCOLN      US   \n",
      "2020-04-07 20:00:00+00:00                  Oxford St Ebbes     Oxford      GB   \n",
      "2020-04-07 19:00:00+00:00                 BROADWAY (South)  St. Louis      US   \n",
      "2020-04-07 18:30:00+00:00  Deen Dayal Nagar, Sagar - MPPCB      Sagar      IN   \n",
      "2020-04-07 20:00:00+00:00                        Manglerud       Oslo      NO   \n",
      "\n",
      "                          pollutant  value   unit source_name   latitude  \\\n",
      "timestamp                                                                  \n",
      "2017-01-18 16:00:00+00:00      pm10   7.00  µg/m³      AirNow  35.488400   \n",
      "2020-04-07 20:00:00+00:00       no2  30.00  µg/m³       DEFRA  51.744804   \n",
      "2020-04-07 19:00:00+00:00      pm25   6.10  µg/m³      AirNow  38.542500   \n",
      "2020-04-07 18:30:00+00:00      pm25  23.67  µg/m³       caaqm  23.864016   \n",
      "2020-04-07 20:00:00+00:00      pm10  27.06  µg/m³      Norway  59.898690   \n",
      "\n",
      "                           longitude  averaged_over_in_hours  \n",
      "timestamp                                                     \n",
      "2017-01-18 16:00:00+00:00 -97.090280                    1.00  \n",
      "2020-04-07 20:00:00+00:00  -1.260278                    1.00  \n",
      "2020-04-07 19:00:00+00:00 -90.263610                    1.00  \n",
      "2020-04-07 18:30:00+00:00  78.802895                    0.25  \n",
      "2020-04-07 20:00:00+00:00  10.814950                    1.00  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Wrong number of items passed 4, placement implies 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-801b40043cf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Dataset sebelum di-downsampling (5 teratas):\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgaq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# [1] Downsampling dari daily to weekly dan kita hitung maksimum untuk seminggu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mgaq_weekly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgaq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'W'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Downsampling daily to weekly - max (5 teratas):\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgaq_weekly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# [2] Downsampling dari daily to quaterly dan kita hitung minimumnya untuk tiap quarter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\resample.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(self, _method, min_count, *args, **kwargs)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         \u001b[0mnv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_resampler_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_downsample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGroupBy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\resample.py\u001b[0m in \u001b[0;36m_downsample\u001b[1;34m(self, how, **kwargs)\u001b[0m\n\u001b[0;32m   1078\u001b[0m         \u001b[1;31m# we are downsampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1079\u001b[0m         \u001b[1;31m# we want to call the actual grouper method here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1080\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_loffset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    943\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_mangle_lambdas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 945\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    946\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\aggregation.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(obj, arg, *args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_aggregate_string_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    564\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[0marg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAggFuncTypeDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_try_aggregate_string_function\u001b[1;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[1;31m# people may try to aggregate on a non-callable attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36mmax\u001b[1;34m(self, numeric_only, min_count)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1677\u001b[0m         return self._agg_general(\n\u001b[1;32m-> 1678\u001b[1;33m             \u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"max\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1679\u001b[0m         )\n\u001b[0;32m   1680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_agg_general\u001b[1;34m(self, numeric_only, min_count, alias, npfunc)\u001b[0m\n\u001b[0;32m   1026\u001b[0m                     \u001b[0malt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnpfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m                     \u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1028\u001b[1;33m                     \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1029\u001b[0m                 )\n\u001b[0;32m   1030\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mDataError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[1;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[0;32m   1014\u001b[0m     ) -> DataFrame:\n\u001b[0;32m   1015\u001b[0m         agg_mgr = self._cython_agg_blocks(\n\u001b[1;32m-> 1016\u001b[1;33m             \u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m         )\n\u001b[0;32m   1018\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_agged_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magg_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magg_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36m_cython_agg_blocks\u001b[1;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[0;32m   1109\u001b[0m         \u001b[1;31m#  continue and exclude the block\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[1;31m# NotImplementedError -> \"ohlc\" with wrong dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1111\u001b[1;33m         \u001b[0mnew_mgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_split_op_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Block\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36m_split_op_result\u001b[1;34m(self, result)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBlock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[1;34m(self, values, placement)\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_block_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_block_same_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[1;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[0;32m   2709\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatetimeArray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2711\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, values, placement, ndim)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_ndim\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             raise ValueError(\n\u001b[1;32m--> 143\u001b[1;33m                 \u001b[1;34mf\"Wrong number of items passed {len(self.values)}, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m                 \u001b[1;34mf\"placement implies {len(self.mgr_locs)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: Wrong number of items passed 4, placement implies 6"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load dataset https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv\n",
    "gaq = pd.read_csv('https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv')\n",
    "gaq['timestamp'] = pd.to_datetime(gaq['timestamp'])\n",
    "gaq = gaq.set_index('timestamp')\n",
    "print('Dataset sebelum di-downsampling (5 teratas):\\n', gaq.head())\n",
    "# [1] Downsampling dari daily to weekly dan kita hitung maksimum untuk seminggu\n",
    "gaq_weekly = gaq.resample('W').max()\n",
    "print('Downsampling daily to weekly - max (5 teratas):\\n', gaq_weekly.head())\n",
    "# [2] Downsampling dari daily to quaterly dan kita hitung minimumnya untuk tiap quarter\n",
    "gaq_quaterly = gaq.resample('Q').min()\n",
    "print('Downsampling daily to quaterly - min (5 teratas):\\n', gaq_quaterly.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsampling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sebelum di-upsampling (5 teratas):\n",
      "                                                   location       city country  \\\n",
      "timestamp                                                                       \n",
      "2017-01-18 16:00:00+00:00                  MOBILE-KICKAPOO    LINCOLN      US   \n",
      "2020-04-07 20:00:00+00:00                  Oxford St Ebbes     Oxford      GB   \n",
      "2020-04-07 19:00:00+00:00                 BROADWAY (South)  St. Louis      US   \n",
      "2020-04-07 18:30:00+00:00  Deen Dayal Nagar, Sagar - MPPCB      Sagar      IN   \n",
      "2020-04-07 20:00:00+00:00                        Manglerud       Oslo      NO   \n",
      "\n",
      "                          pollutant  value   unit source_name   latitude  \\\n",
      "timestamp                                                                  \n",
      "2017-01-18 16:00:00+00:00      pm10   7.00  µg/m³      AirNow  35.488400   \n",
      "2020-04-07 20:00:00+00:00       no2  30.00  µg/m³       DEFRA  51.744804   \n",
      "2020-04-07 19:00:00+00:00      pm25   6.10  µg/m³      AirNow  38.542500   \n",
      "2020-04-07 18:30:00+00:00      pm25  23.67  µg/m³       caaqm  23.864016   \n",
      "2020-04-07 20:00:00+00:00      pm10  27.06  µg/m³      Norway  59.898690   \n",
      "\n",
      "                           longitude  averaged_over_in_hours  \n",
      "timestamp                                                     \n",
      "2017-01-18 16:00:00+00:00 -97.090280                    1.00  \n",
      "2020-04-07 20:00:00+00:00  -1.260278                    1.00  \n",
      "2020-04-07 19:00:00+00:00 -90.263610                    1.00  \n",
      "2020-04-07 18:30:00+00:00  78.802895                    0.25  \n",
      "2020-04-07 20:00:00+00:00  10.814950                    1.00  \n",
      "Upsampling daily to hourly - mean (5 teratas):\n",
      "                            value  latitude  longitude  averaged_over_in_hours\n",
      "timestamp                                                                    \n",
      "2014-05-13 03:00:00+00:00    9.7    14.635     121.08                     1.0\n",
      "2014-05-13 04:00:00+00:00    NaN       NaN        NaN                     NaN\n",
      "2014-05-13 05:00:00+00:00    NaN       NaN        NaN                     NaN\n",
      "2014-05-13 06:00:00+00:00    NaN       NaN        NaN                     NaN\n",
      "2014-05-13 07:00:00+00:00    NaN       NaN        NaN                     NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load dataset https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv\n",
    "gaq = pd.read_csv('https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv')\n",
    "gaq['timestamp'] = pd.to_datetime(gaq['timestamp'])\n",
    "gaq = gaq.set_index('timestamp')\n",
    "print('Dataset sebelum di-upsampling (5 teratas):\\n', gaq.head())\n",
    "# Upsampling dari daily to hourly dan kita hitung reratanya\n",
    "gaq_hourly = gaq.resample('H').mean()\n",
    "print('Upsampling daily to hourly - mean (5 teratas):\\n', gaq_hourly.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling by Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sebelum di-resampling (5 teratas):\n",
      "                                                   location       city country  \\\n",
      "timestamp                                                                       \n",
      "2017-01-18 16:00:00+00:00                  MOBILE-KICKAPOO    LINCOLN      US   \n",
      "2020-04-07 20:00:00+00:00                  Oxford St Ebbes     Oxford      GB   \n",
      "2020-04-07 19:00:00+00:00                 BROADWAY (South)  St. Louis      US   \n",
      "2020-04-07 18:30:00+00:00  Deen Dayal Nagar, Sagar - MPPCB      Sagar      IN   \n",
      "2020-04-07 20:00:00+00:00                        Manglerud       Oslo      NO   \n",
      "\n",
      "                          pollutant  value   unit source_name   latitude  \\\n",
      "timestamp                                                                  \n",
      "2017-01-18 16:00:00+00:00      pm10   7.00  µg/m³      AirNow  35.488400   \n",
      "2020-04-07 20:00:00+00:00       no2  30.00  µg/m³       DEFRA  51.744804   \n",
      "2020-04-07 19:00:00+00:00      pm25   6.10  µg/m³      AirNow  38.542500   \n",
      "2020-04-07 18:30:00+00:00      pm25  23.67  µg/m³       caaqm  23.864016   \n",
      "2020-04-07 20:00:00+00:00      pm10  27.06  µg/m³      Norway  59.898690   \n",
      "\n",
      "                           longitude  averaged_over_in_hours  \n",
      "timestamp                                                     \n",
      "2017-01-18 16:00:00+00:00 -97.090280                    1.00  \n",
      "2020-04-07 20:00:00+00:00  -1.260278                    1.00  \n",
      "2020-04-07 19:00:00+00:00 -90.263610                    1.00  \n",
      "2020-04-07 18:30:00+00:00  78.802895                    0.25  \n",
      "2020-04-07 20:00:00+00:00  10.814950                    1.00  \n",
      "Resampling daily to 2 monthly - mean - ffill (5 teratas):\n",
      "                            value  latitude  longitude  averaged_over_in_hours\n",
      "timestamp                                                                    \n",
      "2014-05-31 00:00:00+00:00    9.7    14.635    121.080                     1.0\n",
      "2014-07-31 00:00:00+00:00    7.1    37.132    -86.148                     1.0\n",
      "2014-09-30 00:00:00+00:00    7.1    37.132    -86.148                     1.0\n",
      "2014-11-30 00:00:00+00:00   18.1   -34.560    -58.506                     1.0\n",
      "2015-01-31 00:00:00+00:00   18.1   -34.560    -58.506                     1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load dataset https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv\n",
    "gaq = pd.read_csv('https://dqlab-dataset.s3-ap-southeast-1.amazonaws.com/LO4/global_air_quality_4000rows.csv')\n",
    "gaq['timestamp'] = pd.to_datetime(gaq['timestamp'])\n",
    "gaq = gaq.set_index('timestamp')\n",
    "print('Dataset sebelum di-resampling (5 teratas):\\n', gaq.head())\n",
    "# Resample dari daily to 2 monthly, hitung reratanya, dan fillna = 'bfill'\n",
    "gaq_2monthly = gaq.resample('2M').mean().fillna(method='bfill')\n",
    "print('Resampling daily to 2 monthly - mean - ffill (5 teratas):\\n', gaq_2monthly.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
