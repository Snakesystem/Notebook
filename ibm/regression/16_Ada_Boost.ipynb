{"cells":[{"cell_type":"markdown","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/images/IDSNlogo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"  />\n","</center>\n","\n","# Adaptive Boosting (AdaBoost) for classification with Python\n","\n","Estimated time needed: **45** minutes\n","\n","## Objectives\n","\n","After completing this lab you will be able to:\n","\n","*   Understand  that AdaBoost is a linear combination of  ùëá weak classifiers\n","*   Apply AdaBoost\n","*   Understand Hyperparameters selection in  AdaBoost\n"]},{"cell_type":"markdown","metadata":{},"source":["In this notebook, you will learn AdaBoost, short for Adaptive Boosting, is a classification algorithm; AdaBoost is actually part of a family of Boosting algorithms. Like Bagging and Random Forest (RF), AdaBoost combines the outputs of many classifiers into an ensemble, but there are some differences. In both Bagging and RF, each classifier in the ensemble is powerful but prone to overfitting. As Bagging or RF aggregate more and more classifiers, they reduce overfitting.\n","\n","With AdaBoost, each Classifier usually has performance slightly better than random. This is referred to as a weak learner or weak classifier. AdaBoost combines these classifiers to get a strong classifier. Unlike Bagging and Random Forest, in AdaBoost, adding more learners can cause overfitting. As a result, AdaBoost requires Hyperparameter tuning, taking more time to train. One advantage of AdaBoost is that each classifier is smaller, so predictions are faster.\n"]},{"cell_type":"markdown","metadata":{},"source":["In AdaBoost, the strong classifier $H(x)$ is a linear combination of $T$ weak classifiers $h_t(x)$ and $\\alpha_t$ as shown in (1). Although each classifier $h_t(x)$ appears independent, the $\\alpha_t$ contains information about the error of classifiers from $h\\_1(x),.., h\\_{t-1}(x)$. As we add more classifiers, the training accuracy gets larger. What‚Äôs not so apparent in (1) is that during the training process, the values of that training sample are modified for $h_t(x)$. For a more in depth look at the theory behind Adaboost, check out <a href=\"https://hastie.su.domains/Papers/ESLII.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork31576874-2022-01-01#page=356\">The Elements of Statistical Learning Data Mining, Inference, and Prediction</a>.\n"]},{"cell_type":"markdown","metadata":{},"source":["$H(x) = \text{sign}(  \\sum\\_{t=1}^T \\alpha_t h_t(x) )$ \\[1]\n"]},{"cell_type":"markdown","metadata":{},"source":["<h1>Table of contents</h1>\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","    <ol>\n","        <li><a href=\"https://#RFvsBag\">What's the difference between RF and Bagging </a></li>\n","        <li><a href=\"https://#Example\">Cancer Data Example</li>\n","        <li><a href=\"https://practice/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork31576874-2022-01-01\">Practice</a></li>\n","\n","</div>\n","<br>\n","<hr>\n"]},{"cell_type":"markdown","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"source":["Let's first import the required libraries:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","# !mamba install -qy pandas==1.3.3 numpy==1.21.2 ipywidgets==7.4.2 scipy==7.4.2 tqdm==4.62.3 matplotlib==3.5.0 seaborn==0.9.0\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["import pandas as pd\n","import pylab as plt\n","import numpy as np\n","import scipy.optimize as opt\n","from sklearn import preprocessing\n","%matplotlib inline \n","import matplotlib.pyplot as plt\n","from sklearn import metrics\n","from tqdm import tqdm\n"]},{"cell_type":"markdown","metadata":{},"source":["Ignore error warnings\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["This function will calculate the accuracy of the training and testing data given a model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_accuracy(X_train, X_test, y_train, y_test, model):\n","    return  {\"test Accuracy\":metrics.accuracy_score(y_test, model.predict(X_test)),\"trian Accuracy\": metrics.accuracy_score(y_train, model.predict(X_train))}"]},{"cell_type":"markdown","metadata":{},"source":["This function calculates the average accuracy of differnt learning rates on training and test data\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_accuracy_bag(X,y,title,times=20,xlabel='Number Estimators',Learning_rate_=[0.2,0.4,0.6,1]):\n","\n","    lines_array=['solid','--', '-.', ':']\n","\n","    N_estimators=[n for n in range(1,100)]\n","    \n","    times=20\n","    train_acc=np.zeros((times,len(Learning_rate_),len(N_estimators)))\n","    test_acc=np.zeros((times,len(Learning_rate_),len(N_estimators)))\n","\n","\n","    #Iterate through different number of Learning rate  and average out the results  \n","    for n in tqdm(range(times)):\n","        X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3)\n","        for n_estimators in N_estimators:\n","            for j,lr in enumerate(Learning_rate_):\n","\n","\n","                model = AdaBoostClassifier(n_estimators=n_estimators+1,random_state=0,learning_rate=lr)\n","\n","\n","                model.fit(X_train,y_train)\n","\n","\n","\n","                Accuracy=get_accuracy(X_train, X_test, y_train, y_test,  model)\n","\n","\n","\n","                train_acc[n,j,n_estimators-1]=Accuracy['trian Accuracy']\n","                test_acc[n,j,n_estimators-1]=Accuracy['test Accuracy']\n","\n","\n","\n","\n","    fig, ax1 = plt.subplots()\n","    mean_test=test_acc.mean(axis=0)\n","    mean_train=train_acc.mean(axis=0)\n","    ax2 = ax1.twinx()\n","\n","    for j,(lr,line) in enumerate(zip(Learning_rate_,lines_array)): \n","\n","        ax1.plot(mean_train[j,:],linestyle = line,color='b',label=\"Learning rate \"+str(lr))\n","        ax2.plot(mean_test[j,:],linestyle = line, color='r',label=str(lr))\n","\n","    ax1.set_ylabel('Training accuracy',color='b')\n","    ax1.legend()\n","    ax2.set_ylabel('Testing accuracy', color='r')\n","    ax2.legend()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"source":["### About the dataset\n","\n","We will use a telecommunications dataset for predicting customer churn. This is a historical customer dataset where each row represents one customer. The data is relatively easy to understand, and you may uncover insights you can use immediately. Typically, it is less expensive to keep customers than to acquire new ones, so the focus of this analysis is to predict the customers who will stay with the company.\n","\n","This data set provides information to help you predict what behavior will help you to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\n","\n","The dataset includes information about:\n","\n","*   Customers who left within the last month ‚Äì the column is called Churn\n","*   Services that each customer has signed up for ‚Äì phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n","*   Customer account information ‚Äì how long they have been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n","*   Demographic info about customers ‚Äì gender, age range, and if they have partners and dependents\n"]},{"cell_type":"markdown","metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"source":["Load Data From CSV File\n"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["churn_df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/ChurnData.csv\")\n","\n","churn_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Data pre-processing and selection\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's select some features for the modeling. Also, we change the target data type to be an integer, as it is a requirement by the skitlearn algorithm:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip', 'callcard', 'wireless','churn']]\n","churn_df['churn'] = churn_df['churn'].astype('int')\n","churn_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Select Variables at Random\n"]},{"cell_type":"markdown","metadata":{},"source":["Like Bagging, RF uses an independent bootstrap sample from the training data. In addition, we select $m$ variables at random out of all $M$ possible\n","variables. Let's do an example.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X=churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']]"]},{"cell_type":"markdown","metadata":{},"source":["there are 7 features\n"]},{"cell_type":"markdown","metadata":{},"source":["## Train/Test dataset\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's define X, and y for our dataset:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y = churn_df['churn']\n","y.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Train/Test dataset\n"]},{"cell_type":"markdown","metadata":{},"source":["We split our dataset into train and test set:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=1)\n","print ('Train set', X_train.shape,  y_train.shape)\n","print ('Test set', X_test.shape,  y_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## AdaBoost\n"]},{"cell_type":"markdown","metadata":{},"source":["We can import the AdaBoost Classifier in Sklearn\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import AdaBoostClassifier"]},{"cell_type":"markdown","metadata":{},"source":["The parameter <code>n_estimatorsint</code> is the maximum number of classifiers (default=50) at which boosting is stopped. If the results are perfect, the training procedure is stopped early.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_estimators=5\n","random_state=0"]},{"cell_type":"markdown","metadata":{},"source":["We can create a <code>AdaBoostClassifier</code> object.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = AdaBoostClassifier(n_estimators=n_estimators,random_state=random_state)"]},{"cell_type":"markdown","metadata":{},"source":["If the outputs were y-1 and 1, the form of the classifier would be:\n"]},{"cell_type":"markdown","metadata":{},"source":["$H(x) = \text{sign}(  \\alpha\\_1 h\\_1(x)+ \\alpha\\_2 h\\_2(x)+ \\alpha\\_3 h\\_3(x)+ \\alpha\\_4 h\\_4(x)+ \\alpha\\_5 h\\_5(x) )$\n"]},{"cell_type":"markdown","metadata":{},"source":["We can fit the object finding all the $\\alpha_t$ $h_t(x)$ and then make a prediction:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","y_pred "]},{"cell_type":"markdown","metadata":{},"source":["We can find the training and testing accuracy:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(get_accuracy(X_train, X_test, y_train, y_test,  model))"]},{"cell_type":"markdown","metadata":{},"source":["We see the base model is a Decision Tree. Since it only has one layer, it‚Äôs called a stump:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.base_estimator_"]},{"cell_type":"markdown","metadata":{},"source":["We can output a list of the weak classifiers: $h\\_1(x), h\\_2(x), h\\_3(x),h\\_4(x)$ and $h\\_5(x)$\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.estimators_"]},{"cell_type":"markdown","metadata":{},"source":["We see the weak classifiers do not perform as well:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["[ (\"for weak classifiers {} the we get \".format(i+1),get_accuracy(X_train, X_test, y_train, y_test,  weak_classifiers)) for i,weak_classifiers in enumerate(model.estimators_)]"]},{"cell_type":"markdown","metadata":{},"source":["We can increase the number of weak classifiers:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_estimators=100\n","random_state=0"]},{"cell_type":"markdown","metadata":{},"source":["and then fit the model\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = AdaBoostClassifier(n_estimators=n_estimators,random_state=random_state)\n","model.fit(X_train, y_train)\n","\n","#Predict the response for test dataset\n","y_pred = model.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["We obtain the  training and testing accuracy:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(get_accuracy(X_train, X_test, y_train, y_test, model))"]},{"cell_type":"markdown","metadata":{},"source":["We see that adding more weak classifiers causes overfitting. We can verify by plotting the training and test accuracy over the number of classifiers:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_accuracy_bag(X,y,title=\"Training and Test Accuracy vs Weak Classifiers\",Learning_rate_=[1],times=20,xlabel='Number Estimators')"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, as the number of classifiers increases so does the overfitting; the training accuracy increases and conversely, the testing accuracy decreases. One way to decrease overfitting is using the learning rate <code>learning_rate</code> with a default value of 1. This is a type of Regularization. For more detail on Regularization, check out <a href=\"https://hastie.su.domains/Papers/ESLII.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork31576874-2022-01-01#page=383\">here</a>.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_estimators=100\n","random_state=0\n","learning_rate=0.7"]},{"cell_type":"markdown","metadata":{},"source":["We can now train the model, make a prediction, and calculate the accuracy. We see that by increasing the learning rate the test accuracy has improved.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = AdaBoostClassifier(n_estimators=n_estimators,random_state=random_state,learning_rate=learning_rate)\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","print(get_accuracy(X_train, X_test, y_train, y_test, model))"]},{"cell_type":"markdown","metadata":{},"source":["Compared to the previous results we see the model does better on the test data. We can try different learning rates using the method <code>get_accuracy_bag</code>. In this case, the learning rates are 0.2,0.4,0.6, and 1. As the learning rate goes down we see that the testing accuracy increases while conversely, the training accuracy decreases .\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_accuracy_bag(X,y,title=\"Training and Test Accuracy vs Weak Classifiers\",Learning_rate_=[0.2,0.4,0.6,1],times=20,xlabel='Number Estimators')"]},{"cell_type":"markdown","metadata":{},"source":["Another important parameter is <code>algorithm</code> with takes on the values <code>SAMME</code>, <code>SAMME.R</code>. The default is  <code>‚ÄôSAMME.R‚Äô</code>. The <code>SAMME.R</code> algorithm typically converges faster than <code>SAMME</code>, achieving a lower test error with fewer boosting iterations. For more details, check <a href=\"https://hastie.su.domains/Papers/samme.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork31576874-2022-01-01\">the paper</a>. One issue is that <code>SAMME.R</code> can't be used all the time as we will need the Base classifier to generate the probability of belonging to each class.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Changing the Base Classifier\n"]},{"cell_type":"markdown","metadata":{},"source":["AdaBoost is usually performed with Decision Trees but we can use other base classifiers. However, if the classifier is too strong it will cause overfitting. Consider using the following Support Vector Machine (SVM) as the base classifier:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.svm import SVC"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["base_estimator=SVC(kernel='rbf',gamma=1)"]},{"cell_type":"markdown","metadata":{},"source":["We see the SVM does extremely well:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["base_estimator.fit(X_train, y_train)\n","\n","print(get_accuracy(X_train, X_test, y_train, y_test, base_estimator))"]},{"cell_type":"markdown","metadata":{},"source":["The Base classifier for SVM can not generate the probability of belonging to each class. If you uncomment the following line of code you will get an error.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#base_estimator.predict_proba(X_train)"]},{"cell_type":"markdown","metadata":{},"source":["Therefore the parameter <code>algorithm</code> must be <code>'SAMME'</code>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["algorithm='SAMME'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model =AdaBoostClassifier(n_estimators=5, base_estimator=base_estimator,learning_rate=1,algorithm='SAMME' )"]},{"cell_type":"markdown","metadata":{},"source":["We see the AdaBoost model does worse, this is similar to overfitting. In addition, SVM takes much longer to train than classification trees:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.fit(X_train, y_train)\n","\n","#Predict the response for test dataset\n","y_pred = model.predict(X_test)\n","print(get_accuracy(X_train, X_test, y_train, y_test, model))"]},{"cell_type":"markdown","metadata":{},"source":["We see that each tree in RF is less correlated than Bagging:\n"]},{"cell_type":"markdown","metadata":{},"source":["<h2 id=\"Example\">Cancer Data Example</h2>\n","\n","The example is based on a dataset that is publicly available from the UCI Machine Learning Repository (Asuncion and Newman, 2007)\\[[http://mlearn.ics.uci.edu/MLRepository.html](http://mlearn.ics.uci.edu/MLRepository.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork31576874-2022-01-01)]. The dataset consists of several hundred human cell sample records, each of which contains the values of a set of cell characteristics. The fields in each record are:\n","\n","| Field name  | Description                 |\n","| ----------- | --------------------------- |\n","| ID          | Clump thickness             |\n","| Clump       | Clump thickness             |\n","| UnifSize    | Uniformity of cell size     |\n","| UnifShape   | Uniformity of cell shape    |\n","| MargAdh     | Marginal adhesion           |\n","| SingEpiSize | Single epithelial cell size |\n","| BareNuc     | Bare nuclei                 |\n","| BlandChrom  | Bland chromatin             |\n","| NormNucl    | Normal nucleoli             |\n","| Mit         | Mitoses                     |\n","| Class       | Benign or malignant         |\n","\n","<br>\n","<br>\n","\n","Let's load the dataset:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/cell_samples.csv\")\n","\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["Now lets remove rows that have a ? in the <code>BareNuc</code> column:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df= df[pd.to_numeric(df['BareNuc'], errors='coerce').notnull()]"]},{"cell_type":"markdown","metadata":{},"source":["We obtain the features:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X =  df[['Clump', 'UnifSize', 'UnifShape', 'MargAdh', 'SingEpiSize', 'BareNuc', 'BlandChrom', 'NormNucl', 'Mit']]\n","\n","X.head()"]},{"cell_type":"markdown","metadata":{},"source":["We obtain the class labels:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y=df['Class']\n","y.head()"]},{"cell_type":"markdown","metadata":{},"source":["We split the data into training and testing sets.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n","print ('Train set:', X_train.shape,  y_train.shape)\n","print ('Test set:', X_test.shape,  y_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["We use <code>GridSearchCV</code> to search over specified parameter values of the model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV"]},{"cell_type":"markdown","metadata":{},"source":["We create a <code>AdaBoost</code> object and list the parameters using the method <code>get_params()</code>:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = AdaBoostClassifier()\n","model.get_params().keys()"]},{"cell_type":"markdown","metadata":{},"source":["We can use GridSearch for Exhaustive search over specified parameter values. We see many of the parameters are similar to Classification trees; let's try different parameters for <code>learning_rate</code>, <code>n_estimators</code>, and <code>algorithm</code>.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["param_grid = {'learning_rate': [0.1*(n+1) for n in range(10)],\n","             'n_estimators' : [2*n+1 for n in range(10)],\n","              'algorithm':['SAMME', 'SAMME.R']}                \n","\n","param_grid "]},{"cell_type":"markdown","metadata":{},"source":["We create the Grid Search object and fit it:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["search = GridSearchCV(estimator=model, param_grid=param_grid,scoring='accuracy')\n","search.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["We can see the best accuracy score of the searched parameters was \\~96%.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["search.best_score_"]},{"cell_type":"markdown","metadata":{},"source":["The best parameter values are:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["search.best_params_"]},{"cell_type":"markdown","metadata":{},"source":["We can calculate accuracy on the test data using the test data:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","print(get_accuracy(X_train, X_test, y_train, y_test, search.best_estimator_))"]},{"cell_type":"markdown","metadata":{},"source":["<h2 id=\"practice\">Practice</h2>\n"]},{"cell_type":"markdown","metadata":{},"source":["Imagine that you are a medical researcher compiling data for a study. You have collected data about a set of patients, all of whom suffered from the same illness. During their course of treatment, each patient responded to one of 5 medications, Drug A, Drug B, Drug c, Drug x and y.\n","\n","Part of your job is to build a model to find out which drug might be appropriate for a future patient with the same illness. The features of this dataset are Age, Sex, Blood Pressure, and the Cholesterol of the patients, and the target is the drug that each patient responded to.\n","\n","It is a sample of multiclass classifier, and you can use the training part of the dataset to build a decision tree, and then use it to predict the class of a unknown patient, or to prescribe a drug to a new patient.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/data/drug200.csv\", delimiter=\",\")\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["Let's create the X and y for our dataset:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = df[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']].values\n","X[0:5]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y = df[\"Drug\"]\n","y[0:5]"]},{"cell_type":"markdown","metadata":{},"source":["Now lets use a <code>LabelEncoder</code> to turn categorical features into numerical:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn import preprocessing\n","le_sex = preprocessing.LabelEncoder()\n","le_sex.fit(['F','M'])\n","X[:,1] = le_sex.transform(X[:,1]) \n","\n","\n","le_BP = preprocessing.LabelEncoder()\n","le_BP.fit([ 'LOW', 'NORMAL', 'HIGH'])\n","X[:,2] = le_BP.transform(X[:,2])\n","\n","\n","le_Chol = preprocessing.LabelEncoder()\n","le_Chol.fit([ 'NORMAL', 'HIGH'])\n","X[:,3] = le_Chol.transform(X[:,3]) \n","\n","X[0:5]"]},{"cell_type":"markdown","metadata":{},"source":["Split the data into training and testing data with a 80/20 split.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n","print ('Train set:', X_train.shape,  y_train.shape)\n","print ('Test set:', X_test.shape,  y_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["We can use GridSearch for Exhaustive search over specified parameter values.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["param_grid = {'learning_rate': [0.1*(n+1) for n in range(10)],\n","             'n_estimators' : [2*n+1 for n in range(10)],\n","              'algorithm':['SAMME', 'SAMME.R']}                \n","\n","param_grid \n"]},{"cell_type":"markdown","metadata":{},"source":["Create a <code>AdaBoostClassifier</code> object called <code>model</code> :\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = AdaBoostClassifier()"]},{"cell_type":"markdown","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","model = RandomForestClassifier()\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["Create <code>GridSearchCV</code> object called `search` with the `estimator` set to <code>model</code>, <code>param_grid</code> set to <code>param_grid</code>, <code>scoring</code> set to <code>accuracy</code>, and <code>cv</code> set to 3 and Fit the <code>GridSearchCV</code> object to our <code>X_train</code> and <code>y_train</code> data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["search = GridSearchCV(estimator=model, param_grid=param_grid,scoring='accuracy', cv=3)\n","search.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","search = GridSearchCV(estimator=model, param_grid=param_grid,scoring='accuracy', cv=3)\n","search.fit(X_train, y_train)\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["We can find the accuracy of the best model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["search.best_score_"]},{"cell_type":"markdown","metadata":{},"source":["We can find the best parameter values:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["search.best_params_"]},{"cell_type":"markdown","metadata":{},"source":["We can find the accuracy test data:\n"]},{"cell_type":"markdown","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","print(get_accuracy(X_train, X_test, y_train, y_test, search.best_estimator_))\n","```\n","\n","</details>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(get_accuracy(X_train, X_test, y_train, y_test, search.best_estimator_))"]},{"cell_type":"markdown","metadata":{},"source":["### Thank you for completing this lab!\n","\n","## Author\n","\n","<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\">Joseph Santarcangelo</a>\n","\n","### Other Contributors\n","\n","<a href=\"https://www.linkedin.com/in/richard-ye/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\">Richard Ye</a>\n","\n","## Change Log\n","\n","| Date (YYYY-MM-DD) | Version | Changed By           | Change Description   |\n","| ----------------- | ------- | -------------------- | -------------------- |\n","| 2020-11-27        | 0.1     | Joseph Santarcangelo | Created Lab Template |\n","| 2022-2-8          | 0.2     | Steve Hord           | QA pass              |\n","| 2022-05-03        | 0.3     | Richard Ye           | Fixed spelling/HTML  |\n","\n","## <h3 align=\"center\"> ¬© IBM Corporation 2020. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"widgets":{"state":{},"version":"1.1.2"}},"nbformat":4,"nbformat_minor":4}